{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0e6c52-e3da-46fc-8d8a-b2727e22dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "register_heif_opener()\n",
    "\n",
    "from similarities import ClipSimilarity, SiftSimilarity\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import fnmatch\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3798669-5081-4503-90f3-3d1fd963c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingletonModelLoader:\n",
    "    _instances = {}\n",
    "\n",
    "    def __new__(cls, model_name_or_path):\n",
    "        if model_name_or_path not in cls._instances:\n",
    "            instance = super(SingletonModelLoader, cls).__new__(cls)\n",
    "            instance.model = ClipSimilarity(model_name_or_path=model_name_or_path)\n",
    "            cls._instances[model_name_or_path] = instance\n",
    "        return cls._instances[model_name_or_path]\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, model_name_or_path):\n",
    "        return cls(model_name_or_path).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bef5bf4-ef90-49f1-86a3-f31ab9846989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "class CacheManager:\n",
    "\n",
    "    ROOT_BASE = '~/.cache/ai_album/'\n",
    "    \n",
    "    def __init__(self, cache_path_prefix, root_path, cache_tag, generate_func, format_str=\"{base}_{cache_tag}.cache\"):\n",
    "        self.cache_path_prefix = cache_path_prefix\n",
    "        self.root_path = os.path.abspath(root_path)\n",
    "        self.cache_tag = cache_tag\n",
    "        self.generate_func = generate_func\n",
    "        self.format_str = format_str\n",
    "\n",
    "    def _get_cache_file_path(self, path):\n",
    "\n",
    "        root_p, root_folder_name = os.path.split(self.root_path.rstrip('/'))\n",
    "        cache_path = os.path.abspath(path).replace(root_p, self.ROOT_BASE + self.cache_path_prefix)\n",
    "\n",
    "        base, ext = os.path.splitext(cache_path)\n",
    "        ext = ext[1:]\n",
    "        p = self.format_str.format(base=base, ext=ext, cache_tag=self.cache_tag)\n",
    "        \n",
    "        return os.path.expanduser(p).lower()\n",
    "\n",
    "    def load(self, path):\n",
    "        def save(data, path):\n",
    "            if isinstance(data, Image.Image):\n",
    "                data.save(path, quality=50)\n",
    "            elif isinstance(data, str):\n",
    "                with open(path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(data)\n",
    "            else:\n",
    "                with open(path, 'wb') as file:\n",
    "                    pickle.dump(data, file)\n",
    "\n",
    "        def load_individual_file(path):\n",
    "            if path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
    "                return Image.open(path)\n",
    "            elif path.lower().endswith('.txt'):\n",
    "                with open(path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()\n",
    "            else:\n",
    "                with open(path, 'rb') as file:\n",
    "                    return pickle.load(file)\n",
    "\n",
    "        cache_file_path = self._get_cache_file_path(path)\n",
    "\n",
    "        # Check if cache file exists or if it matches any files when wildcard is present\n",
    "        if '*' in cache_file_path:\n",
    "            matched_files = glob.glob(cache_file_path)\n",
    "            if not matched_files:\n",
    "                os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)\n",
    "                i = 0\n",
    "                for item in self.generate_func(path):\n",
    "                    i += 1\n",
    "                    item_path = cache_file_path.replace('*', str(i))\n",
    "                    save(item, item_path)\n",
    "                    \n",
    "            matched_files = glob.glob(cache_file_path)\n",
    "            return [load_individual_file(file_path) for file_path in sorted(matched_files)]\n",
    "        else:\n",
    "            if not os.path.exists(cache_file_path):\n",
    "                data = self.generate_func(path)\n",
    "                os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)\n",
    "                save(data, cache_file_path)\n",
    "\n",
    "            if os.path.exists(cache_file_path):\n",
    "                return load_individual_file(cache_file_path)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd31a39-0b7e-48fe-b5c0-d1085824f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = 10\n",
    "\n",
    "def format_filename(file_path):\n",
    "    \"\"\"Truncate and format the file name to a maximum of 30 characters.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return (file_name[:27] + '...') if len(file_name) > 30 else file_name\n",
    "\n",
    "class VideoManager:\n",
    "    def __init__(self, folder_path, model_name='OFA-Sys/chinese-clip-vit-huge-patch14'):\n",
    "        self.folder_path = folder_path\n",
    "        self.model_name = model_name.replace('/', '_')\n",
    "        self.similarity_model = SingletonModelLoader.get_model(model_name)\n",
    "        self.frame_cache_manager = CacheManager(cache_path_prefix=\".similarity_cache/video/\",\n",
    "                                                root_path=folder_path,\n",
    "                                                cache_tag=\"frames\",\n",
    "                                                generate_func=self._extract_and_cache_frames,\n",
    "                                                format_str='{base}/thumbnail_{cache_tag}_*.jpg')\n",
    "        self.emb_cache_manager = CacheManager(cache_path_prefix=\".similarity_cache/video/\",\n",
    "                                              root_path=folder_path,\n",
    "                                              cache_tag=\"emb\",\n",
    "                                              generate_func=self._generate_embeddings,\n",
    "                                              format_str='{base}/sim_{cache_tag}_*.emb')\n",
    "\n",
    "    def extract_key_frame(self, path):\n",
    "        file_name = format_filename(path)\n",
    "        print(f\"Extracting frames from video '{file_name}'...\")\n",
    "\n",
    "        embeddings = self.emb_cache_manager.load(path)\n",
    "\n",
    "        max_avg_similarity = 0\n",
    "        key_frame = None\n",
    "        pil_frames = self.frame_cache_manager.load(path)\n",
    "\n",
    "        for i, emb_a in enumerate(embeddings):\n",
    "            total_similarity = 0\n",
    "            for j, emb_b in enumerate(embeddings):\n",
    "                if i != j:\n",
    "                    similarity = self.similarity_model.score_functions['cos_sim'](emb_a, emb_b)\n",
    "                    total_similarity += similarity\n",
    "\n",
    "            avg_similarity = total_similarity / (len(embeddings) - 1)\n",
    "            if avg_similarity > max_avg_similarity:\n",
    "                max_avg_similarity = avg_similarity\n",
    "                key_frame = pil_frames[i]\n",
    "\n",
    "        return key_frame\n",
    "\n",
    "    def extract_frames(self, video_path):\n",
    "        return self.frame_cache_manager.load(video_path)\n",
    "\n",
    "    def _generate_embeddings(self, video_path):\n",
    "        frames = self.extract_frames(video_path)\n",
    "        batch_embeddings = self.similarity_model.get_embeddings(frames, show_progress_bar=True, batch_size=8)\n",
    "        return batch_embeddings\n",
    "\n",
    "    def _extract_and_cache_frames(self, video_path):\n",
    "        cache_dir = self._create_cache_directory(video_path)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_interval = int(fps * INTERVAL)\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_count % frame_interval == 0 or frame_count == total_frames - 1:\n",
    "                frame_path = os.path.join(cache_dir, f\"frame_{frame_count}.jpg\")\n",
    "                pil_image = self._cv_frame_to_pil_image(frame)\n",
    "                pil_image.thumbnail((1280, 720))\n",
    "                yield pil_image\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    def _create_cache_directory(self, video_path):\n",
    "        cache_dir = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        cache_dir = f\".similarity_cache/video/{cache_dir}_cache\"\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        return cache_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def _cv_frame_to_pil_image(frame):\n",
    "        cv2_image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(cv2_image_rgb)\n",
    "        return pil_image\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146acce8-0c58-4b8e-b59d-c2b4091c91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarity:\n",
    "    def __init__(self, folder_path, model_name='OFA-Sys/chinese-clip-vit-huge-patch14', batch_size=8, show_progress_bar=True, **kwargs):\n",
    "        print(\"Initializing ImageSimilarity...\")\n",
    "\n",
    "        self.model_name = model_name.replace('/', '_')  # Replace '/' in model name for file paths\n",
    "        self.similarity_model = SingletonModelLoader.get_model(model_name)\n",
    "        self.batch_size = batch_size\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "        self.kwargs = kwargs  # Store any additional keyword arguments\n",
    "\n",
    "        self.folder_path = folder_path\n",
    "        self.media_fps = self._load_image_paths(folder_path)\n",
    "        self.video_mng = VideoManager(folder_path)\n",
    "        self.qa = ImageQuestionAnswerer(\"blip_caption\", \"large_coco\")\n",
    "\n",
    "        # Initialize CacheManagers\n",
    "        self.thumbnail_cache_manager = CacheManager(cache_path_prefix=\".similarity_cache/img/\",\n",
    "                                                    root_path=folder_path,\n",
    "                                                    cache_tag=\"thumbnail\",\n",
    "                                                    generate_func=self._compute_and_save_thumbnail,\n",
    "                                                    format_str=\"{base}_thumbnail.jpg\")\n",
    "        self.embedding_cache_manager = CacheManager(cache_path_prefix=\".similarity_cache/img/\",\n",
    "                                                    root_path=folder_path,\n",
    "                                                    cache_tag=\"emb\",\n",
    "                                                    generate_func=self._generate_embedding,\n",
    "                                                    format_str=\"{base}.emb\")\n",
    "        self.caption_cache_manager = CacheManager(cache_path_prefix=\".similarity_cache/img/\",\n",
    "                                                    root_path=folder_path,\n",
    "                                                    cache_tag=\"caption\",\n",
    "                                                    generate_func=self._generate_caption,\n",
    "                                                    format_str=\"{base}_caption.txt\")\n",
    "\n",
    "        print(\"Loaded similarity model and image file paths.\")\n",
    "        self._initialize()\n",
    "        self.similarity_cache = self._cache_similarities()\n",
    "        print(\"Initialization complete.\")\n",
    "\n",
    "    def _load_image_paths(self, folder_path):\n",
    "        img_fps = sorted(os.path.join(root, f) for root, _, files in os.walk(folder_path) for f in files if self._is_image(f))\n",
    "        vid_fps = sorted(os.path.join(root, f) for root, _, files in os.walk(folder_path) for f in files if self._is_video(f))\n",
    "\n",
    "        return img_fps + vid_fps\n",
    "\n",
    "    def _is_image(self, path):\n",
    "        file_extensions = ['*.jpg', '*.jpeg', '*.png', '*.heic', '*.heif']\n",
    "        return any(fnmatch.fnmatch(path.lower(), ext) for ext in file_extensions)\n",
    "        \n",
    "    def _is_video(self, path):\n",
    "        file_extensions = ['*.mp4', '*.avi', '*.webm', '*.mkv', '*.mov']\n",
    "        return any(fnmatch.fnmatch(path.lower(), ext) for ext in file_extensions)\n",
    "\n",
    "    def _compute_and_save_thumbnail(self, image_path):\n",
    "        def compute_thumbnail(path):\n",
    "            if self._is_image(path):\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.thumbnail((1280, 720))\n",
    "                    return img\n",
    "\n",
    "            if self._is_video(path):\n",
    "                return self.video_mng.extract_key_frame(path)\n",
    "\n",
    "            return None\n",
    "\n",
    "        return compute_thumbnail(image_path)\n",
    "        \n",
    "    def _initialize(self):\n",
    "        print(\"Initializing embeddings...\")\n",
    "        for fp in tqdm(self.media_fps, desc=\"Initializing embeddings\"):\n",
    "            _ = self.embedding_cache_manager.load(fp)\n",
    "    \n",
    "        print(\"Initializing captions...\")\n",
    "        for fp in tqdm(self.media_fps, desc=\"Initializing captions\"):\n",
    "            _ = self.caption_cache_manager.load(fp)\n",
    "\n",
    "    def _generate_embedding(self, image_path):\n",
    "        img = self.thumbnail_cache_manager.load(image_path)\n",
    "        emb = self.similarity_model.get_embeddings([img])[0]  # Extract the first (and only) embedding\n",
    "        return emb\n",
    "\n",
    "    def _generate_caption(self, image_path):\n",
    "        img = self.thumbnail_cache_manager.load(image_path)\n",
    "        return self.qa.caption(img, max_length=90, min_length=30)[0]\n",
    "    \n",
    "    def _cache_similarities(self):\n",
    "        cache = {}\n",
    "        batch_size = 100\n",
    "\n",
    "        print(\"Caching similarities...\")\n",
    "        pbar = tqdm(total=len(self.media_fps), desc=\"Caching similarities\")\n",
    "        for i in range(0, len(self.media_fps), batch_size):\n",
    "            end_idx = min(i + batch_size, len(self.media_fps))\n",
    "            batch_fps = self.media_fps[i:end_idx]\n",
    "\n",
    "            # Load embeddings for the batch\n",
    "            embeddings = [self.embedding_cache_manager.load(fp) for fp in batch_fps]\n",
    "\n",
    "            # Compute similarity matrix for the batch\n",
    "            for j in range(len(batch_fps)):\n",
    "                for k in range(j + 1, len(batch_fps)):  # Avoid duplicate computations\n",
    "                    sim_score = self.similarity_model.score_functions['cos_sim'](embeddings[j], embeddings[k])\n",
    "                    cache[(i + j, i + k)] = sim_score\n",
    "                    cache[(i + k, i + j)] = sim_score\n",
    "\n",
    "            pbar.update(min(batch_size, len(self.media_fps) - i))\n",
    "\n",
    "        pbar.close()\n",
    "        return cache\n",
    "        \n",
    "    def get_similarity_with_file_path(self, file_path_a, file_path_b):\n",
    "        if file_path_a in self.media_fps and file_path_b in self.media_fps:\n",
    "            idx_a = self.media_fps.index(file_path_a)\n",
    "            idx_b = self.media_fps.index(file_path_b)\n",
    "            return self.similarity_cache.get((idx_a, idx_b), None)\n",
    "        else:\n",
    "            return None  # File path not found\n",
    "\n",
    "    def cluster_images_with_multilevel_hierarchical(self, distance_levels=None):\n",
    "        \"\"\"\n",
    "        Cluster embeddings in a multi-level hierarchy using a list of distance thresholds.\n",
    "\n",
    "        :param embeddings: The embeddings to cluster.\n",
    "        :param distance_levels: A list of distance thresholds for each level of clustering.\n",
    "        :return: Nested dictionary representing multi-level hierarchical clusters.\n",
    "        \"\"\"\n",
    "        embeddings = [self.embedding_cache_manager.load(fp) for fp in self.media_fps]\n",
    "        \n",
    "        if distance_levels is None:\n",
    "            distance_levels = [0.05]  # Default value\n",
    "\n",
    "        # Pair each embedding with its corresponding file path\n",
    "        paired_data = list(zip(self.media_fps, embeddings))\n",
    "\n",
    "        # Starting with all paired data as the initial cluster\n",
    "        initial_cluster = {0: paired_data}\n",
    "\n",
    "        # Function to recursively apply clustering\n",
    "        def recursive_clustering(current_clusters, level):\n",
    "            if level >= len(distance_levels):\n",
    "                # At the final level, return the file paths instead of (file path, embedding) pairs\n",
    "                return {cluster_id: [fp for fp, _ in cluster_data] for cluster_id, cluster_data in current_clusters.items()}\n",
    "\n",
    "            new_clusters = {}\n",
    "            for cluster_id, cluster_data in current_clusters.items():\n",
    "                if len(cluster_data) > 1:\n",
    "                    # Extract embeddings for clustering\n",
    "                    cluster_embeddings = [emb for _, emb in cluster_data]\n",
    "                    clustering = AgglomerativeClustering(distance_threshold=distance_levels[level], n_clusters=None)\n",
    "                    clustering.fit(cluster_embeddings)\n",
    "\n",
    "                    sub_clusters = {}\n",
    "                    for idx, label in enumerate(clustering.labels_):\n",
    "                        sub_clusters.setdefault(label, []).append(cluster_data[idx])\n",
    "\n",
    "                    new_clusters[cluster_id] = recursive_clustering(sub_clusters, level + 1)\n",
    "                else:\n",
    "                    # If only one item in cluster, no need for further clustering\n",
    "                    new_clusters[cluster_id] = cluster_data\n",
    "\n",
    "            return new_clusters\n",
    "\n",
    "        # Apply recursive clustering starting from level 0\n",
    "        final_clusters = recursive_clustering(initial_cluster, 0)\n",
    "        return final_clusters\n",
    "            \n",
    "\n",
    "    def cluster_images_with_hierarchical(self, embeddings, distance_threshold=0.05):\n",
    "        '''The best. distance_threshold = 0.5 for detailed cluster. distance_threshold = 2 for coarse cluster'''\n",
    "        # Convert list of embeddings to a numpy array\n",
    "        embeddings_array = np.array(embeddings)\n",
    "\n",
    "        # Apply Hierarchical Clustering\n",
    "        hierarchical_cluster = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage='ward')\n",
    "        hierarchical_cluster.fit(embeddings_array)\n",
    "\n",
    "        # Extract cluster assignments\n",
    "        labels = hierarchical_cluster.labels_\n",
    "\n",
    "        # Group file paths by cluster labels\n",
    "        clusters = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            clusters.setdefault(label, []).append(self.media_fps[idx])\n",
    "\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6a1c0-debe-4952-a328-40671c994e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ImageSimilarity...\n",
      "Loaded similarity model and image file paths.\n",
      "Initializing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing embeddings: 100%|██████████████████████████████████████| 100/100 [00:00<00:00, 1344.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing captions:   3%|█▎                                          | 3/100 [01:49<57:28, 35.55s/it]"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "# folder_path = './data/YeonWooPinkBikini/'\n",
    "folder_path = './data/blue_minidress/'\n",
    "s = ImageSimilarity(folder_path,\n",
    "                    model_name='OFA-Sys/chinese-clip-vit-huge-patch14',\n",
    "                    # model_name='OFA-Sys/chinese-clip-vit-base-patch16',\n",
    "                    batch_size=16,\n",
    "                    show_progress_bar=True)\n",
    "# file_path_a = './data/YeonWooPinkBikini/Yeon-Woo-Pink-Bikini-telegram[asiansts]-021.jpg'\n",
    "# file_path_b = './data/YeonWooPinkBikini/Yeon-Woo-Pink-Bikini-telegram[asiansts]-031.jpg'\n",
    "# print(s.get_similarity_with_file_path(file_path_a, file_path_b))  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d24d9e5-ae9e-4d87-a73f-ff832a9f1d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: {0: ['./data/blue_minidress/IMG_2718.HEIC',\n",
      "             './data/blue_minidress/IMG_2719.HEIC',\n",
      "             './data/blue_minidress/IMG_2720.HEIC',\n",
      "             './data/blue_minidress/IMG_2721.HEIC',\n",
      "             './data/blue_minidress/IMG_2722.HEIC',\n",
      "             './data/blue_minidress/IMG_2726.HEIC'],\n",
      "         1: ['./data/blue_minidress/IMG_2661.HEIC',\n",
      "             './data/blue_minidress/IMG_2662.HEIC',\n",
      "             './data/blue_minidress/IMG_2663.HEIC',\n",
      "             './data/blue_minidress/IMG_2664.HEIC',\n",
      "             './data/blue_minidress/IMG_2665.HEIC',\n",
      "             './data/blue_minidress/IMG_2666.HEIC',\n",
      "             './data/blue_minidress/IMG_2667.HEIC',\n",
      "             './data/blue_minidress/IMG_2668.HEIC',\n",
      "             './data/blue_minidress/IMG_2669.HEIC',\n",
      "             './data/blue_minidress/IMG_2670.HEIC'],\n",
      "         2: ['./data/blue_minidress/IMG_2723.HEIC',\n",
      "             './data/blue_minidress/IMG_2724.HEIC',\n",
      "             './data/blue_minidress/IMG_2725.HEIC']},\n",
      "     1: {0: ['./data/blue_minidress/IMG_2024.HEIC',\n",
      "             './data/blue_minidress/IMG_2022.MOV'],\n",
      "         1: ['./data/blue_minidress/IMG_2008.HEIC',\n",
      "             './data/blue_minidress/IMG_2011.HEIC',\n",
      "             './data/blue_minidress/IMG_2012.HEIC',\n",
      "             './data/blue_minidress/IMG_2013.HEIC',\n",
      "             './data/blue_minidress/IMG_2020.HEIC',\n",
      "             './data/blue_minidress/IMG_2021.HEIC'],\n",
      "         2: ['./data/blue_minidress/IMG_2034.HEIC',\n",
      "             './data/blue_minidress/IMG_2035.HEIC',\n",
      "             './data/blue_minidress/IMG_2036.HEIC',\n",
      "             './data/blue_minidress/IMG_2037.HEIC',\n",
      "             './data/blue_minidress/IMG_2038.HEIC',\n",
      "             './data/blue_minidress/IMG_2039.HEIC'],\n",
      "         3: ['./data/blue_minidress/IMG_2023.HEIC',\n",
      "             './data/blue_minidress/IMG_2031.HEIC',\n",
      "             './data/blue_minidress/IMG_2032.HEIC',\n",
      "             './data/blue_minidress/IMG_2033.HEIC'],\n",
      "         4: ['./data/blue_minidress/IMG_2016.HEIC',\n",
      "             './data/blue_minidress/IMG_2017.HEIC',\n",
      "             './data/blue_minidress/IMG_2018.HEIC',\n",
      "             './data/blue_minidress/IMG_2019.HEIC'],\n",
      "         5: ['./data/blue_minidress/IMG_2042.HEIC',\n",
      "             './data/blue_minidress/IMG_2043.HEIC',\n",
      "             './data/blue_minidress/IMG_2044.HEIC',\n",
      "             './data/blue_minidress/IMG_2045.HEIC'],\n",
      "         6: ['./data/blue_minidress/IMG_2009.HEIC',\n",
      "             './data/blue_minidress/IMG_2010.HEIC'],\n",
      "         7: ['./data/blue_minidress/IMG_2040.HEIC',\n",
      "             './data/blue_minidress/IMG_2041.HEIC'],\n",
      "         8: ['./data/blue_minidress/IMG_2014.HEIC',\n",
      "             './data/blue_minidress/IMG_2015.HEIC'],\n",
      "         9: ['./data/blue_minidress/IMG_2025.HEIC',\n",
      "             './data/blue_minidress/IMG_2026.HEIC',\n",
      "             './data/blue_minidress/IMG_2027.HEIC',\n",
      "             './data/blue_minidress/IMG_2028.HEIC',\n",
      "             './data/blue_minidress/IMG_2029.HEIC',\n",
      "             './data/blue_minidress/IMG_2030.HEIC']},\n",
      "     2: {0: ['./data/blue_minidress/IMG_2842.HEIC',\n",
      "             './data/blue_minidress/IMG_2847.MOV'],\n",
      "         1: ['./data/blue_minidress/IMG_2839.HEIC',\n",
      "             './data/blue_minidress/IMG_2844.HEIC',\n",
      "             './data/blue_minidress/IMG_2844.MOV'],\n",
      "         2: ['./data/blue_minidress/IMG_2840.HEIC',\n",
      "             './data/blue_minidress/IMG_2841.HEIC',\n",
      "             './data/blue_minidress/IMG_2843.HEIC'],\n",
      "         3: ['./data/blue_minidress/IMG_2845.JPG',\n",
      "             './data/blue_minidress/IMG_2846.JPG']},\n",
      "     3: {0: ['./data/blue_minidress/IMG_2007.HEIC',\n",
      "             './data/blue_minidress/IMG_2071.HEIC'],\n",
      "         1: ['./data/blue_minidress/IMG_2050.HEIC',\n",
      "             './data/blue_minidress/IMG_2051.HEIC',\n",
      "             './data/blue_minidress/IMG_2049.MOV',\n",
      "             './data/blue_minidress/IMG_2063.MOV'],\n",
      "         2: ['./data/blue_minidress/IMG_2072.HEIC',\n",
      "             './data/blue_minidress/IMG_2073.HEIC',\n",
      "             './data/blue_minidress/IMG_2074.MOV'],\n",
      "         3: ['./data/blue_minidress/IMG_2058.JPG',\n",
      "             './data/blue_minidress/IMG_2059.JPG',\n",
      "             './data/blue_minidress/IMG_2060.JPG',\n",
      "             './data/blue_minidress/IMG_2061.JPG',\n",
      "             './data/blue_minidress/IMG_2062.JPG'],\n",
      "         4: ['./data/blue_minidress/IMG_2064.HEIC',\n",
      "             './data/blue_minidress/IMG_2065.HEIC',\n",
      "             './data/blue_minidress/IMG_2066.HEIC',\n",
      "             './data/blue_minidress/IMG_2067.HEIC',\n",
      "             './data/blue_minidress/IMG_2068.HEIC',\n",
      "             './data/blue_minidress/IMG_2069.HEIC'],\n",
      "         5: ['./data/blue_minidress/IMG_2070.MOV'],\n",
      "         6: ['./data/blue_minidress/IMG_2075.MOV',\n",
      "             './data/blue_minidress/IMG_2076.MOV'],\n",
      "         7: ['./data/blue_minidress/IMG_2052.HEIC',\n",
      "             './data/blue_minidress/IMG_2053.HEIC',\n",
      "             './data/blue_minidress/IMG_2054.HEIC',\n",
      "             './data/blue_minidress/IMG_2055.HEIC',\n",
      "             './data/blue_minidress/IMG_2056.HEIC',\n",
      "             './data/blue_minidress/IMG_2057.HEIC'],\n",
      "         8: ['./data/blue_minidress/IMG_2077.MOV'],\n",
      "         9: ['./data/blue_minidress/IMG_2046.JPG',\n",
      "             './data/blue_minidress/IMG_2047.JPG',\n",
      "             './data/blue_minidress/IMG_2048.JPG']}}}\n"
     ]
    }
   ],
   "source": [
    "# clusters = s.cluster_images_with_hierarchical(distance_threshold=0.5)\n",
    "clusters = s.cluster_images_with_multilevel_hierarchical(distance_levels=[2, 0.5])\n",
    "pprint.pprint(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a92a61-6f2b-452d-b8b4-cdacf88b0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_files_to_clusters(clusters, target_path, current_path=\"\"):\n",
    "    for cluster_id, contents in clusters.items():\n",
    "        # Create a new subdirectory for the current cluster\n",
    "        cluster_dir = os.path.join(target_path, current_path, str(cluster_id))\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "        if isinstance(contents, dict):\n",
    "            # If the contents are a dictionary, recurse into it\n",
    "            copy_files_to_clusters(contents, target_path, os.path.join(current_path, str(cluster_id)))\n",
    "        elif isinstance(contents, list):\n",
    "            # If the contents are a list, copy the files into the current cluster directory\n",
    "            for file_path in contents:\n",
    "                shutil.copy(file_path, cluster_dir)\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'clusters' is your multi-level hierarchical cluster dictionary\n",
    "copy_files_to_clusters(clusters, './data/testoutput/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b8e02-d915-4642-bc22-43e3271c84bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d31d9e9a-603f-44fc-bfd0-350bbdfbfb4a",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17bf19e0-99e1-42fd-b0b7-fe53c88584a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"explicit_content\": \"Does this photo contain explicit content?\",\n",
    "    \"cloth_color\": \"which color or colors or nude does the girl wear?\",\n",
    "    \"camera_angle\": \"which angle does the photo take from according to the girl?\",\n",
    "    \"pose\": \"which pose does the girl perform?\",\n",
    "    \"is_sex\": \"is the main character in the picture having sex currently\",\n",
    "    \"is_blowjob\": \"is it doing a oral sex?\",\n",
    "    \"is_doggy\": \"is it having sex in doggy style from behind?\",\n",
    "    \"is_virginia_penetration\": \"is there virginia penetrated by penis?\",\n",
    "    \"pose_hand\": \"what is the pose of her hand\",\n",
    "    \"pose_thigh\": \"what is the pose of her thigh\",\n",
    "    \"pose_crotch\": \"what is the pose of her crotch\",\n",
    "    \"pose_leg\": \"what is the pose of her leg\",\n",
    "    \"name\": \"give a informative filename for this photo that make it unique\",\n",
    "    # Add other keywords and questions as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58135e13-4ca5-4a08-b9e1-c28366c63752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "# qa = ImageQuestionAnswerer(\"blip_vqa\", \"vqav2\")\n",
    "qa = ImageQuestionAnswerer(\"blip_caption\", \"large_coco\")\n",
    "\n",
    "# qa = ImageQuestionAnswerer(\"blip2_opt\", \"caption_coco_opt2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14bb8447-c912-415a-8714-eee2a8197f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2050.HEIC')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2661.HEIC')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2066.HEIC')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2840.HEIC')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2726.HEIC')\n",
    "img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2077.MOV')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2076.MOV')\n",
    "# img = s.thumbnail_cache_manager.load('./data/blue_minidress/IMG_2007.HEIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fc37080-f912-4257-9568-a7737a01845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/blue_minidress/IMG_2007.HEIC ['a woman in a gray dress standing in a living room']\n",
      "./data/blue_minidress/IMG_2008.HEIC ['a woman sitting on a window sill in front of a window']\n",
      "./data/blue_minidress/IMG_2009.HEIC ['a woman sitting on a window sill in front of a window']\n",
      "./data/blue_minidress/IMG_2010.HEIC ['a woman is sitting on a window sill']\n",
      "./data/blue_minidress/IMG_2011.HEIC ['a woman is sitting on a window sill']\n",
      "./data/blue_minidress/IMG_2012.HEIC ['a woman sitting on a window sill in a room']\n",
      "./data/blue_minidress/IMG_2013.HEIC ['a woman is sitting on a window sill']\n",
      "./data/blue_minidress/IMG_2014.HEIC ['a woman is sitting on a window sill']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mmedia_fps:\n\u001b[0;32m----> 2\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaption\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthumbnail_cache_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fp, res)\n",
      "Cell \u001b[0;32mIn[52], line 40\u001b[0m, in \u001b[0;36mImageQuestionAnswerer.caption\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcaption\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     39\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_img(img)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/blip_models/blip_caption.py:179\u001b[0m, in \u001b[0;36mBlipCaption.generate\u001b[0;34m(self, samples, use_nucleus_sampling, num_beams, max_length, min_length, top_p, repetition_penalty, num_captions)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    samples (dict): A dictionary containing the following keys:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m'the famous singapore fountain at sunset']\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# prepare inputs for decoder generation.\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(encoder_out, num_captions, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    182\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt] \u001b[38;5;241m*\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/blip_models/blip_caption.py:54\u001b[0m, in \u001b[0;36mBlipCaption.forward_encoder\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[0;32m---> 54\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_embeds\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/vit.py:527\u001b[0m, in \u001b[0;36mVisionTransformerEncoder.forward_features\u001b[0;34m(self, x, register_blk)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, register_blk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_blk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/vit.py:278\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, register_blk)\u001b[0m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_blk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/vit.py:157\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, register_hook)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, register_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    156\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), register_hook\u001b[38;5;241m=\u001b[39mregister_hook))\n\u001b[0;32m--> 157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/lavis/models/vit.py:49\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ai_album/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fp in s.media_fps:\n",
    "    res = qa.caption(s.thumbnail_cache_manager.load(fp))\n",
    "    print(fp, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b98010f6-d6cb-44d1-bdd7-de25b6af4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/blue_minidress/IMG_2007.HEIC ['a woman in a gray dress standing in a living room next to a white couch and a window with a view of a city outside']\n",
      "./data/blue_minidress/IMG_2008.HEIC ['a woman sitting on a window sill in front of a window with a view of a snowy mountain outside of her window and a coffee table']\n",
      "./data/blue_minidress/IMG_2009.HEIC ['a woman sitting on a window sill in front of a large window with a view of a city and a coffee table in the corner']\n",
      "./data/blue_minidress/IMG_2010.HEIC ['a woman sitting on a window sill in a room with a view of a city and a coffee table and a window sill']\n",
      "./data/blue_minidress/IMG_2011.HEIC ['a woman sitting on a window sill in a room with a view of a mountain outside of the window and a pair of shoes on the window sill']\n",
      "./data/blue_minidress/IMG_2012.HEIC ['a woman sitting on a window sill next to a coffee table and a pair of shoes on the floor in front of a window']\n",
      "./data/blue_minidress/IMG_2013.HEIC ['a woman sitting on a window sill next to a window sill with a cup of coffee on it and a green umbrella in the background']\n",
      "./data/blue_minidress/IMG_2014.HEIC ['a woman sitting on a window sill with a cup of tea in her hand and a cup of tea in her hand on the other hand']\n",
      "./data/blue_minidress/IMG_2015.HEIC ['a woman sitting on a window sill with a cup of tea in her hand and a cup of tea in her hand on the other hand']\n",
      "./data/blue_minidress/IMG_2016.HEIC ['a woman sitting on a window sill in front of a window with a green umbrella in the window sill and a table with pillows']\n",
      "./data/blue_minidress/IMG_2017.HEIC ['a woman sitting on a window sill with her feet on a pair of shoes in front of a window with a view of a snowy landscape']\n",
      "./data/blue_minidress/IMG_2018.HEIC ['a woman sitting on a window sill in a room with a view of a snowy mountain outside of the window and a coffee table']\n",
      "./data/blue_minidress/IMG_2019.HEIC ['a woman sitting on a window sill in front of a window with a green umbrella in the window sill next to a couch']\n",
      "./data/blue_minidress/IMG_2020.HEIC ['a woman sitting on a window sill next to a table with a cup of coffee and a pair of shoes on the window sill']\n",
      "./data/blue_minidress/IMG_2021.HEIC ['a woman sitting on a window sill in a room with a view of a mountain and a coffee cup on a table in front of the window']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fp in s.media_fps:\n",
    "    res = qa.caption(s.thumbnail_cache_manager.load(fp), max_length=90, min_length=30)\n",
    "    print(fp, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0af9e7b6-7e57-4661-b7e8-e1a8f949e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this photo contain explicit content? ['yes']\n",
      "which color or colors or nude does the girl wear? ['gray']\n",
      "which angle does the photo take from according to the girl? ['straight']\n",
      "which pose does the girl perform? ['back']\n",
      "is the main character in the picture having sex currently ['yes']\n",
      "is it doing a oral sex? ['no']\n",
      "is it having sex in doggy style from behind? ['yes']\n",
      "is there virginia penetrated by penis? ['yes']\n",
      "what is the pose of her hand ['on her butt']\n",
      "what is the pose of her thigh ['bent']\n",
      "what is the pose of her crotch ['bent']\n",
      "what is the pose of her leg ['bent']\n",
      "give a informative filename for this photo that make it unique ['naked woman']\n",
      "CPU times: user 40.6 s, sys: 3.32 s, total: 43.9 s\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k, v in questions.items():\n",
    "    res = qa.ask(img, v)\n",
    "    print(v, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c02695-2f73-4a48-9c59-13eb2e0887d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f04126-068b-4924-a6dc-fab9d9c23678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this photo contain explicit content? yes\n",
      "which color or colors or nude does the girl wear? brown\n",
      "which angle does the photo take from according to the girl? sideways\n",
      "which pose does the girl perform? woman is laying down\n",
      "is the main character in the picture having sex currently yes\n",
      "is it doing a blowjob? yes\n",
      "is it having sex in doggy style position? yes\n",
      "CPU times: user 22 s, sys: 5.67 s, total: 27.7 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ask a question about an image\n",
    "tmp = [(k,v) for k, v in questions.items()]\n",
    "res = qa.asks(img, [k for k, v in tmp])\n",
    "for (_, q), a in zip(tmp, res):\n",
    "    print(q, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcd929b3-55bc-4c6c-b90b-2ee99d4c9cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Back (180 Degrees)']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.rank(img, 'which angle is the photo taken from based on the girl', \n",
    "        [\n",
    "            'Front (0 degree)',\n",
    "            'Side Profile (90 Degrees)',\n",
    "            'Back (180 Degrees)',\n",
    "            'Slight Side Back (150 Degrees)',\n",
    "            'Three-Quarter View (45 Degrees)',\n",
    "            'Slight Side Angle (15-30 Degrees)',\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1fa5d60-1f0b-481f-95c1-9fdcc90c746b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low angle']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.rank(img, 'which angle is the photo taken from based on the girl', \n",
    "        [\n",
    "            \"eye level\",\n",
    "            \"high angle\",\n",
    "            \"low angle\",\n",
    "            \"bird's eye view\",\n",
    "            \"worm's eye view\",\n",
    "            \"over the shoulder shot\",\n",
    "            \"close up\",\n",
    "            \"frog view\",\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea981f6-f255-4e72-81d1-f20b2a9e4130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
